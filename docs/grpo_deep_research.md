# GRPO and Fine-Tuning LLMs for Mathematical Reasoning

## 1. GRPO for Fine-Tuning LLMs (vs. PPO and DPO)
**What is GRPO?** Group Relative Policy Optimization (GRPO) is a reinforcement learning algorithm introduced in the context of improving LLMs’ reasoning abilities ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=code%20data,the%20memory%20usage%20of%20PPO)). It was first applied in *DeepSeekMath* (an open 7B model) to boost math problem-solving, helping that model reach ~51.7% accuracy on the challenging MATH benchmark (approaching GPT-4-level performance) ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=code%20data,the%20memory%20usage%20of%20PPO)). GRPO is essentially a variant of Proximal Policy Optimization (PPO) tailored for LLM fine-tuning. The key idea is to generate multiple candidate outputs per prompt and use **intra-group comparisons** to compute advantages, rather than relying on a separate value network ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=modifies%20the%20traditional%20Proximal%20Policy,to%20improve%20models%20on%20helpfulness)) ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=The%20Key%20Differences%20from%20Proximal,PPO%29%20are)). This makes training more memory-efficient and aligned with how preference data are usually collected (comparing outputs for the same query). In practical terms, GRPO optimizes a policy (the LLM) to maximize a reward signal (e.g. solution correctness) *relative* to other outputs in the same group, while penalizing deviation from the original model (via a KL-divergence term) ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=The%20objective%20is%20to%20maximize,i%2Ct%E2%88%92%CE%B2%20D%20KL%5B%CF%80%20%CE%B8%E2%88%A5%CF%80%20ref)) ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=GRPOConfig,i%2Ct%29%E2%88%92%CE%B2%20D%20KL%5B%CF%80%20%CE%B8%E2%88%A5%CF%80%20ref)).

**Improvements over PPO:** GRPO introduces several innovations over standard PPO that are particularly beneficial for mathematical reasoning tasks:

- **No Value Model:** GRPO removes the need for a learned value (critic) network to baseline rewards. Instead, it uses the *average reward of a group* of model outputs as a baseline ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=modifies%20the%20traditional%20Proximal%20Policy,to%20improve%20models%20on%20helpfulness)) ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=The%20Key%20Differences%20from%20Proximal,PPO%29%20are)). This significantly cuts memory and compute overhead, which is important for large models ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=modifies%20the%20traditional%20Proximal%20Policy,Rewards%20as%20well%20as%20General)) ([Group Relative Policy Optimization (GRPO) Illustrated Breakdown | Ebrahim Pichka](https://epichka.com/blog/2025/grpo/#:~:text=1,the%20objective%20and%20the%20rewards)). PPO normally requires training a critic to estimate future reward, but math problem solutions usually have a clear outcome (correct/incorrect) that can be assessed at the end. GRPO sidesteps the critic by computing a relative reward per group of sampled solutions, simplifying training.

- **Group-Based Advantage:** In GRPO, for each prompt the model generates *G* outputs, and their rewards (from a reward function or model) are compared. The reward of each output is normalized by subtracting the mean reward of the group and dividing by the group’s standard deviation ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=For%20each%20of%20the%20G,i%2Ct%3Dstd%28r%29r%20i%E2%88%92mean%28r)) ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=of%20comparisons%20between%20outputs%20for,i%2Ct%3Dstd%28r%29r%20i%E2%88%92mean%28r)). This yields a **group relative advantage** for each output. Intuitively, the model learns to make its answers better than the other candidates for the same question ([
          Policy Gradient Algorithms | RLHF Book by Nathan Lambert
      ](https://rlhfbook.com/c/11-policy-gradients.html#:~:text=Intuitively%2C%20the%20GRPO%20update%20is,specific%20action%20is%20than%20the)). This approach aligns with the comparative nature of many reward signals (e.g. preference models that rank outputs) and leads to more stable improvements on reasoning tasks ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=3,KL%20term%20within%20the%20reward)) ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=,part%20of%20the%20reward%20signal)). By using other sampled solutions as a baseline, the model gets a more informative signal of *how much better or worse* an answer is, which is useful for structured problems like math.

- **Integrated KL Penalty:** GRPO explicitly includes a KL-divergence term in its loss to keep the fine-tuned policy close to the original model (reference policy) ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=The%20Key%20Differences%20from%20Proximal,PPO%29%20are)) ([Group Relative Policy Optimization (GRPO) Illustrated Breakdown | Ebrahim Pichka](https://epichka.com/blog/2025/grpo/#:~:text=1,the%20objective%20and%20the%20rewards)). In PPO-based RLHF, a KL penalty is often implemented as part of the reward or through a separate constraint; GRPO makes it part of the loss function directly. This helps prevent the model from drifting too far and producing unnatural outputs. The GRPO loss is essentially: **loss = –(advantage) + β · KL** (plus PPO-style clipping on the policy update) ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=The%20objective%20is%20to%20maximize,i%2Ct%E2%88%92%CE%B2%20D%20KL%5B%CF%80%20%CE%B8%E2%88%A5%CF%80%20ref)) ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=GRPOConfig,i%2Ct%29%E2%88%92%CE%B2%20D%20KL%5B%CF%80%20%CE%B8%E2%88%A5%CF%80%20ref)). A small coefficient β (e.g. 0.04 by default) is used to weight the KL term ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=vllm_guided_decoding_regex%3A%20typing.Optional,6%20ref_model_sync_steps%3A%20int%20%3D%20512)). This “conservative update” ensures that while the model learns to maximize reward (e.g. getting correct answers), it doesn’t lose its general language fluency or go off-distribution.

Together, these features make GRPO well-suited for mathematical reasoning. Math problems often yield a natural reward signal (correct vs incorrect solution). GRPO capitalizes on this by generating multiple solutions and reinforcing the relative differences. For example, if one solution in the group is correct and others are wrong, GRPO heavily rewards the correct one and penalizes the rest, teaching the model what a correct reasoning chain looks like. This proved effective: DeepSeekMath credited GRPO as a key to its strong math performance ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=code%20data,the%20memory%20usage%20of%20PPO)), and other projects (e.g. DeepSeek-R1 and Qwen-LLM) have since adopted GRPO for reasoning improvements ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=modifies%20the%20traditional%20Proximal%20Policy,to%20improve%20models%20on%20helpfulness)) ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=scores%2C%20reducing%20memory%20usage%20and,to%20improve%20models%20on%20helpfulness)).

**Comparison to DPO:** Direct Preference Optimization (DPO) is an alternative fine-tuning approach where the model is trained directly on preference-ranked pairs without an explicit RL loop. DPO forgoes a reward model and policy sampling; instead, it treats the preferred output as the target and maximizes its log-likelihood over a dispreferred output ([Direct Preference Optimization (DPO) | by João Lages - Medium](https://medium.com/@joaolages/direct-preference-optimization-dpo-622fc1f18707#:~:text=Direct%20Preference%20Optimization%20%28DPO%29%20,tuning%2C%20or)) ([[PDF] Direct Preference Optimization: Your Language Model is ... - arXiv](https://arxiv.org/pdf/2305.18290#:~:text=arXiv%20arxiv,Our)). This makes DPO much simpler and more efficient to implement than PPO/GRPO – there’s no online generation or value network training ([Noteworthy LLM Research Papers of 2024](https://sebastianraschka.com/blog/2025/llm-research-2024.html#:~:text=4,DPO%20used%20today)). In fact, some recent LLMs (e.g. Llama 3) have used DPO for alignment rather than PPO, due to these practical advantages ([Noteworthy LLM Research Papers of 2024](https://sebastianraschka.com/blog/2025/llm-research-2024.html#:~:text=widely%20used%20in%20practice%20than,PPO)). However, DPO’s simplicity can be a drawback on complex reasoning tasks. Research has found that **PPO-style methods typically outperform DPO in terms of raw performance**, especially on out-of-distribution or very challenging data ([Noteworthy LLM Research Papers of 2024](https://sebastianraschka.com/blog/2025/llm-research-2024.html#:~:text=Is%20DPO%20Superior%20to%20PPO,distribution%20data)). DPO tends to lag when the preference data differs from what the model saw during supervised tuning, unless additional finetuning is done first ([Noteworthy LLM Research Papers of 2024](https://sebastianraschka.com/blog/2025/llm-research-2024.html#:~:text=Is%20DPO%20Superior%20to%20PPO,distribution%20data)).

For mathematical reasoning, DPO would require curated preference data (e.g. human or model rankings of solutions). This is non-trivial to obtain at scale for math, whereas defining a reward (like “+1 for correct answer, 0 for incorrect”) is straightforward. GRPO can continuously sample and explore solutions, receiving gradual feedback, whereas DPO is a one-shot training on fixed comparisons. In practice, **GRPO provides a stronger learning signal for math**: it encourages the model to generate correct step-by-step solutions by actively comparing different outcomes. While DPO might be easier to run (no need to generate multiple solutions per query during training), it might not push a model to its full potential on a domain as intricate as math. Indeed, a comprehensive study in 2024 showed PPO-based RLHF still tends to yield better aligned models than DPO in many cases ([Noteworthy LLM Research Papers of 2024](https://sebastianraschka.com/blog/2025/llm-research-2024.html#:~:text=Is%20DPO%20Superior%20to%20PPO,distribution%20data)) ([Noteworthy LLM Research Papers of 2024](https://sebastianraschka.com/blog/2025/llm-research-2024.html#:~:text=4,DPO%20used%20today)). The trade-off is that GRPO/PPO are more complex (needing a reward function and sampling) but can squeeze out extra performance. In summary, **GRPO builds on PPO’s strengths (strong performance, ability to optimize an explicit reward) while addressing its inefficiencies, and it can surpass simpler methods like DPO for challenging reasoning tasks**.

**Using Hugging Face’s GRPO Trainer:** Implementing GRPO has been made convenient by the Hugging Face transformers RL (TRL) library. The TRL library provides a `GRPOTrainer` and `GRPOConfig` class that handle the heavy lifting ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=,from%20trl%20import%20GRPOConfig%2C%20GRPOTrainer)) ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=training_args%20%3D%20GRPOConfig%28output_dir%3D%22Qwen2,train)). To use it, you need: (1) a pretrained model to fine-tune, (2) a **reward function or model** to score outputs, and (3) a dataset of prompts (questions). For example, one can load a base model and tokenizer, wrap the model with LoRA for efficiency (optional), and define a reward function that measures some aspect of the completion (e.g. match to correct answer or any custom metric) ([Practical Exercise: Fine-tune a model with GRPO - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter12/5?fw=pt#:~:text=Define%20the%20reward%20function)) ([Practical Exercise: Fine-tune a model with GRPO - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter12/5?fw=pt#:~:text=def%20reward_len%28completions%2C%20%2A%2Akwargs%29%3A%20return%20%5B,completion%29%29%20for%20completion%20in%20completions)). In Hugging Face’s framework, you then create a `GRPOConfig` with training hyperparameters and instantiate `GRPOTrainer` with the model, reward function(s), config, and dataset ([Practical Exercise: Fine-tune a model with GRPO - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter12/5?fw=pt#:~:text=,per_device_train_batch_size%3D8%2C%20gradient_accumulation_steps%3D2%2C%20max_prompt_length%3D512%2C%20max_completion_length%3D96%2C%20num_generations%3D8)) ([Practical Exercise: Fine-tune a model with GRPO - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter12/5?fw=pt#:~:text=,train)). The trainer will automatically:

- **Generate multiple completions** for each prompt per training step (`num_generations` in the config, e.g. 8) ([Practical Exercise: Fine-tune a model with GRPO - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter12/5?fw=pt#:~:text=training_args%20%3D%20GRPOConfig%28%20output_dir%3D%22GRPO%22%2C%20learning_rate%3D2e,adamw_8bit)). These are the G candidates for comparison.
- **Compute rewards** for each generated output (by calling the provided reward function or model) and normalize them within each group ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=Computing%20the%20advantage)) ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=3,KL%20term%20within%20the%20reward)).
- **Calculate the GRPO loss** for the model’s outputs (advantage term minus KL penalty) and backpropagate. It uses policy gradient updates similar to PPO, optionally with clipping (controlled by ε in the config, default 0.2) ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=The%20objective%20is%20to%20maximize,i%2Ct%E2%88%92%CE%B2%20D%20KL%5B%CF%80%20%CE%B8%E2%88%A5%CF%80%20ref)) ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=GRPOConfig,i%2Ct%29%E2%88%92%CE%B2%20D%20KL%5B%CF%80%20%CE%B8%E2%88%A5%CF%80%20ref)).
- **Maintain a reference model** (usually a copy of the initial policy) to compute the KL divergence. By default the reference is fixed, but the library allows periodic syncing or mix-up with the current policy if specified (to stabilize long runs) ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=vllm_guided_decoding_regex%3A%20typing.Optional,6%20ref_model_sync_steps%3A%20int%20%3D%20512)).

Hugging Face’s GRPOTrainer handles many best practices under the hood. For instance, it supports experience replay (reusing past prompts) and can log metrics like average reward, advantage, and KL divergence for monitoring ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=Logged%20metrics)). In terms of configuration, you can set the learning rate, batch size, max prompt/answer lengths, number of training epochs, etc., just like a standard `Trainer` ([Practical Exercise: Fine-tune a model with GRPO - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter12/5?fw=pt#:~:text=training_args%20%3D%20GRPOConfig%28%20output_dir%3D%22GRPO%22%2C%20learning_rate%3D2e,num_train_epochs%3D1)). A typical setup might use a moderate LR (e.g. 2e-5) with AdamW optimizer (often in 8-bit mode to save memory) ([Practical Exercise: Fine-tune a model with GRPO - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter12/5?fw=pt#:~:text=training_args%20%3D%20GRPOConfig%28%20output_dir%3D%22GRPO%22%2C%20learning_rate%3D2e,adamw_8bit)), generate, say, 4–8 solutions per prompt, and accumulate gradients if needed to reach a larger effective batch. It’s also important to enable mixed precision (fp16 or bfloat16) if available, and to set `remove_unused_columns=False` in the config (since the dataset may not be in a typical text-only format) ([Practical Exercise: Fine-tune a model with GRPO - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter12/5?fw=pt#:~:text=gradient_accumulation_steps%3D2%2C%20max_prompt_length%3D512%2C%20max_completion_length%3D96%2C%20num_generations%3D8%2C%20optim%3D,remove_unused_columns%3DFalse)). Overall, using TRL’s GRPO trainer significantly streamlines the implementation: you focus on writing a good reward function (for math, this might call a math checker or compare to ground truth answers) and the framework takes care of orchestrating the RL fine-tuning loop.

*In summary, GRPO is a cutting-edge RL fine-tuning approach that improves mathematical reasoning by optimizing a model on groupwise comparisons of its own outputs. It retains PPO’s robustness but is more memory-efficient and directly suited to tasks like math where relative answer quality is key. By using Hugging Face’s GRPO tools, one can implement this method to fine-tune models (like Llama 3.2-3B) for math with relative ease, configuring the number of generated solutions, KL penalty, and other hyperparameters for stable training.* ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=code%20data,the%20memory%20usage%20of%20PPO)) ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=The%20Key%20Differences%20from%20Proximal,PPO%29%20are))

## 2. Improving Math Performance in LLMs: Latest Research
Fine-tuning large language models for mathematical reasoning has been an active area of research, with various strategies proposed to boost performance on tasks like arithmetic, algebra, and competition math problems. Here we summarize recent approaches and findings:

- **Continued Pre-Training on Math Data:** One straightforward approach is to further pre-train a model on math-rich corpora before fine-tuning on specific problems. For example, DeepSeekMath 7B was created by continuing the base model’s training on 120 billion tokens of math text (from web data and code) ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=,is%20attributed%20to%20two%20key)). Google’s *Minerva* project similarly took a PaLM model and pre-trained it on a large dataset of scientific and mathematical content. This additional pre-training equips the model with better mathematical knowledge and syntax familiarity. Hendrycks et al. (2021) also released an auxiliary pre-training corpus of math text alongside the MATH dataset to help models “learn the fundamentals of mathematics” ([[2103.03874] Measuring Mathematical Problem Solving With the MATH Dataset](https://arxiv.org/abs/2103.03874#:~:text=,show%20that%20accuracy%20remains%20relatively)). The consensus is that exposing models to a lot of equations, proofs, and problem-solving text in the pre-training stage yields a strong foundation. DeepSeekMath demonstrated the impact: after math-focused pre-training (and GRPO fine-tuning), it achieved over 50% on MATH, a huge jump from base models ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=code%20data,the%20memory%20usage%20of%20PPO)).

- **Chain-of-Thought Supervised Fine-Tuning:** Another crucial method is **supervised fine-tuning (SFT) on step-by-step solutions**, often called chain-of-thought (CoT) fine-tuning. Instead of training the model to directly output the final answer, the model is trained on problems paired with their full solutions (the kind a human would write). The MATH dataset itself provides detailed step-by-step solutions for each problem ([[2103.03874] Measuring Mathematical Problem Solving With the MATH Dataset](https://arxiv.org/abs/2103.03874#:~:text=,show%20that%20accuracy%20remains%20relatively)), which can be used to teach the model to reason through the problem before giving an answer. Fine-tuning on such CoT data has been shown to significantly improve accuracy on math word problems ([Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards](https://arxiv.org/html/2404.10346v4#:~:text=Recent%20works%20have%20shown%20that,2023%3B%20Yu)) ([Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards](https://arxiv.org/html/2404.10346v4#:~:text=reasoning%20over%20multiple%20steps%20is,44)). For instance, a model like GPT-3 (175B) saw dramatic improvements on benchmarks when coerced to produce multi-step reasoning. For smaller open models, recent works (e.g. *MetaMath* by Yu et al., 2023) have compiled collections of math problems with solutions and performed supervised fine-tuning, yielding solid gains. Chain-of-thought fine-tuning essentially trains the model to “show its work,” which not only improves accuracy but also makes debugging easier. It helps mitigate errors by enabling the model to break complex questions into manageable steps.

- **Knowledge Distillation from Larger Models:** Because high-quality human-written solutions are scarce, a popular strategy is to leverage powerful models (like GPT-4) to generate training data for smaller models. This is a form of distillation. Researchers have augmented math datasets by asking GPT-4 or other top models to produce detailed solutions or to annotate existing solutions step by step ([Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards](https://arxiv.org/html/2404.10346v4#:~:text=have%20shown%20limited%20performance%2C%20and,44)) ([Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards](https://arxiv.org/html/2404.10346v4#:~:text=However%2C%20acquiring%20high,51)). For example, *WizardMath* (2023) used an *Evol-Instruct* approach, where they iteratively generated new math problems and solutions using open models and refined them, as well as likely took inspiration from GPT-4 outputs ([WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/html/2308.09583v2#:~:text=Inspired%20by%20Evol,could%20firstly%20generate%20diverse%20math)) ([WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/html/2308.09583v2#:~:text=match%20at%20L129%20instructions%20data,2023%29%2C%20which%20mainly%20focus)). By fine-tuning on these synthetic QA pairs and rationales, smaller LLMs can learn patterns that they didn’t grasp from limited original data. Many entries in recent math LLM leaderboards are products of such distillation. However, reliance on proprietary models can be costly and has an upper-bound (you can’t surpass the teacher model’s quality) ([Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards](https://arxiv.org/html/2404.10346v4#:~:text=However%2C%20acquiring%20high,51)). Still, this method has propelled models from near 0 performance to respectable levels. For instance, distilling GSM8K (a grade-school math dataset) from GPT-4 has enabled 7B models to achieve 80%+ accuracy on it, when they would struggle with vanilla fine-tuning alone.

- **Reinforcement Learning and Reward Optimization:** Beyond supervised learning, reinforcement learning techniques (like RLHF) have been adapted specifically for math reasoning. We discussed GRPO above, which is one such technique that directly optimizes a reward (e.g. solve the problem correctly) by trial-and-error. Another example is *WizardMath’s* Reinforcement Learning from Evol-Instruct Feedback (RLEIF), which combined an evolutionary data generation with RL fine-tuning ([WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/html/2308.09583v2#:~:text=Inspired%20by%20Evol,could%20firstly%20generate%20diverse%20math)) ([WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/html/2308.09583v2#:~:text=match%20at%20L129%20instructions%20data,2023%29%2C%20which%20mainly%20focus)). WizardMath fine-tuned Llama-based models with a mix of instruction-following data and RL feedback (using reward models that checked the correctness of the reasoning process and final answer). This approach led to state-of-the-art results: *WizardMath* reports outperforming all prior open models on both GSM8K and MATH benchmarks ([WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/html/2308.09583v2#:~:text=This%20paper%20introduces%20WizardMath%2C%20a,supervision%20in%20achieving%20exceptional%20performance)). In fact, WizardMath’s 70B model reached performance on par with ChatGPT-3.5 and other proprietary systems on many math tasks ([WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/html/2308.09583v2#:~:text=This%20paper%20introduces%20WizardMath%2C%20a,supervision%20in%20achieving%20exceptional%20performance)). This demonstrates that carefully designed RL (with proper rewards for each step and final answer) can markedly enhance math solving ability. Similarly, DeepSeek-R1 (2025) used multi-stage RL training (with GRPO) on math and coding problems, yielding huge jumps in math competition scores (e.g. boosting a model’s score on the AIME exam from 15.6% to 71.0% after RL training) ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=Image%3A%20prompt)). The takeaway is that **optimizing a model’s behavior with a reward signal, rather than only learning from demonstrations, helps it generalize and achieve higher accuracy in complex reasoning**. The model can explore various solution paths and reinforce those that lead to correct answers or well-structured reasoning.

- **Self-Improvement and Iterative Refinement:** A novel line of research asks if LLMs can *improve themselves* on math tasks by analyzing their own mistakes. One example is the *Self-Explore* method by Hwang et al. (2024). In Self-Explore, the model is fine-tuned through a process of identifying the “first wrong step” in its generated solution and using that as a learning signal ([Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards](https://arxiv.org/html/2404.10346v4#:~:text=However%2C%20acquiring%20human,Our%20code%20is%20available%20here)) ([Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards](https://arxiv.org/html/2404.10346v4#:~:text=To%20this%20end%2C%20we%20propose,Our%20code%20is%20available%20here)). Concretely, the model generates a solution, then detects where it went off track (by trying multiple ways to continue from each step). That first error is treated as a negative example, and a revised pairwise dataset is built to train the model to avoid such errors in the future ([Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards](https://arxiv.org/html/2404.10346v4#:~:text=Image%3A%20Refer%20to%20caption%20Figure,better%20learning%20signal%20during%20training)) ([Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards](https://arxiv.org/html/2404.10346v4#:~:text=Inspired%20by%20prior%20works%20that,the%20rationales%20into%20positive%20and)). This approach provides *fine-grained rewards*: instead of only knowing if the final answer was right or wrong, the model learns which step in reasoning was flawed. Self-Explore improved math performance by about +2.9% on MATH and +11.6% on GSM8K compared to standard fine-tuning ([Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards](https://arxiv.org/html/2404.10346v4#:~:text=problem%20of%20whether%20LLMs%20could,Our%20code%20is%20available%20here)) ([Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards](https://arxiv.org/html/2404.10346v4#:~:text=first%20wrong%20step%20%28i,Our%20code%20is%20available%20here)), across multiple base models. This is a promising direction because it doesn’t require external data – the model iteratively generates and learns from its own failures. Other works have explored *stepwise reward models* or *verifiers* that check each step of a solution (sometimes called “process supervision”). By training models with rewards at intermediate steps (not just the final answer), we can better steer their reasoning. For instance, reinforcing a model whenever it correctly simplifies an equation or applies a theorem correctly, even if the final answer is wrong, can guide learning in long reasoning chains.

- **Multi-Task and Multi-Format Training:** Math problems can be presented and solved in different ways (free-form solutions, multiple-choice answers, formal proofs, programmatic solutions, etc.). Recent research suggests that training on *diverse formats* can improve generalization. A paper in 2024 introduced *MinT (Multi-view Fine-tuning)*, which treated different solution styles as different “views” and had the model learn from all of them ([MinT: Boosting Generalization in Mathematical Reasoning via Multi-view Fine-tuning - ACL Anthology](https://aclanthology.org/2024.lrec-main.988/#:~:text=Reasoning%20in%20mathematical%20domains%20remains,in%20diverse%20formats%20in%20a)) ([MinT: Boosting Generalization in Mathematical Reasoning via Multi-view Fine-tuning - ACL Anthology](https://aclanthology.org/2024.lrec-main.988/#:~:text=avoids%20over,models%20promising%20generalization%20ability%20across)). For example, one view might be a detailed step-by-step explanation, another view might be a concise answer or a sketch of a proof. By appending instructions to prompts indicating the desired format, they fine-tuned a model to produce multiple types of solutions. This multi-view training helped the model handle various problem formats and even learn from partially correct or noisy data ([MinT: Boosting Generalization in Mathematical Reasoning via Multi-view Fine-tuning - ACL Anthology](https://aclanthology.org/2024.lrec-main.988/#:~:text=diverse%20annotation%20styles,view%20training%20paradigm%20could%20inspire)) ([MinT: Boosting Generalization in Mathematical Reasoning via Multi-view Fine-tuning - ACL Anthology](https://aclanthology.org/2024.lrec-main.988/#:~:text=flexible%20manner,in%20other%20machine%20reasoning%20domains)). The result was a smaller model that outperformed others which relied on a single data source or heavy distillation. In general, combining datasets (math word problems, competition problems, coding challenges that require math, etc.) and formats (natural language vs. formal) can strengthen a model’s reasoning flexibility. The model learns underlying math concepts rather than overfitting to one style of question. Many top math-oriented LLMs are trained on a blend of sources – for instance, OpenAI’s GPT-4 was likely trained on code and math texts, enabling it to do arithmetic and algebra internally. Open models like *MathCoder* or *Mistral-Math* also aggregate multiple math-focused datasets.

**Key Methods to Enhance Math Problem-Solving:** Summarizing the above, some **best practices** have emerged for fine-tuning LLMs on math:

- Train the model to use **chain-of-thought reasoning** (either via prompt techniques or by supervised fine-tuning on solutions). This improves correctness on multi-step problems by a large margin ([Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards](https://arxiv.org/html/2404.10346v4#:~:text=Recent%20works%20have%20shown%20that,Li)) ([Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards](https://arxiv.org/html/2404.10346v4#:~:text=a%20rationale%20before%20its%20final,2023%3B%20Yu)).
- Leverage **high-quality solutions from larger models or humans** to overcome data scarcity. Distillation of reasoning patterns can quickly elevate a smaller model’s abilities ([Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards](https://arxiv.org/html/2404.10346v4#:~:text=have%20shown%20limited%20performance%2C%20and,44)) ([Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards](https://arxiv.org/html/2404.10346v4#:~:text=However%2C%20acquiring%20high,51)).
- Use **reinforcement learning or feedback mechanisms** to further push performance once a model has good basic skills. Methods like GRPO, PPO, and custom rewards (e.g. a reward model that checks if the final answer is correct, or if each step is valid) can fine-tune the model toward higher accuracy beyond what supervised data alone achieves ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=1,This%20is%20different)) ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=applied%20GRPO%20to%20unsupervised%20reasoning,like%20format%2C%20mathematics%2C%20and%20coding)).
- Apply **curriculum learning** or staged training: start with easier problems or partial reasoning tasks and gradually move to harder problems. This prevents the model from being overwhelmed by unsolvable prompts early on. For example, one might fine-tune on GSM8K (grade school math) first, then on MATH (competition math), as done by some teams.
- Ensure the model is proficient in the relevant **format and tools** – for instance, if using a calculator or external tool is disallowed, the model must do arithmetic itself. Fine-tuning on numeric calculation exercises or teaching it to break down calculations can help. If external code execution is allowed (as in some setups), fine-tuning the model to write Python code for math problems (as in the PAL method) can greatly improve accuracy – but in our scenario we assume the model itself must do the reasoning.

**Benchmark Findings:** Over the past year, numerous fine-tuned models have been benchmarked on math tasks, yielding some notable comparisons:

- Pure supervised fine-tuning on MATH or GSM8K can get a base LLaMA 7B model from virtually 0% to around 20–30% accuracy on MATH. For instance, one baseline (MetaMath 7B) achieved ~19.8% on MATH ([WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/html/2308.09583v2#:~:text=MAmmoTH,2%207B%2084.1%2043.5)), and another (MathScale 7B) ~31.1% ([WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/html/2308.09583v2#:~:text=Llama,2%207B%2082.6%2040.6)), whereas an *unfine-tuned* Llama-2 7B scored only 2.5% on MATH ([WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/html/2308.09583v2#:~:text=WizardMath,2%207B%2073.2%2021.6)). This gap underscores how much domain-specific fine-tuning improves performance.

- Using chain-of-thought data (like GSM8K’s step-by-step solutions) further boosts results, especially on easier math problems. On GSM8K (grade school math), fine-tuned open models have reached well above 80% accuracy. For example, WizardMath’s 7B variants achieve ~90% on GSM8K ([WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/html/2308.09583v2#:~:text=JiuZhang3.0%C2%A0%28Zhou%20et%C2%A0al.%2C%202024%20%29%20Mistral,hdashline)), whereas base Llama-2 was ~15% ([WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/html/2308.09583v2#:~:text=WizardMath,2%207B%2073.2%2021.6)) on GSM8K. So fine-tuning can transform a model’s capability dramatically on focused tasks.

- Combining supervised and RL training yields the best results. DeepSeekMath 7B, after extensive tuning (data + GRPO RL), hit 51.7% on MATH ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=code%20data,the%20memory%20usage%20of%20PPO)). WizardMath-Mistral 7B (which used evol-instruct + RL) reports ~55–56% on MATH ([WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/html/2308.09583v2#:~:text=Math,Math%207B%2093.9%2077.8)). These are state-of-the-art among 7B-scale models and approach the performance of much larger models. In fact, some specialized 7B models now even exceed older 100B+ models on math. For instance, WizardMath’s latest “Mathstral” model got 70.9% on MATH ([WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/html/2308.09583v2#:~:text=JiuZhang3.0%C2%A0%28Zhou%20et%C2%A0al.%2C%202024%20%29%20Mistral,hdashline)), and a Qwen-7B variant reached 77.8% ([WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/html/2308.09583v2#:~:text=JiuZhang3.0%C2%A0%28Zhou%20et%C2%A0al.%2C%202024%20%29%20Mistral,hdashline)) – numbers that rival GPT-4’s early scores. This shows that targeted fine-tuning can beat raw scale: a well-trained 7B model can outperform a 70B model that wasn’t specifically fine-tuned for math.

- There is evidence that **instruction-tuned models can lose some math ability** unless the data includes math. A study on *faithfulness of reasoning after fine-tuning* noted that general instruction tuning can sometimes degrade multi-step reasoning accuracy (the model starts producing plausible but incorrect reasoning) ([On the Impact of Fine-Tuning on Chain-of-Thought Reasoning - arXiv](https://arxiv.org/html/2411.15382v1#:~:text=On%20the%20Impact%20of%20Fine,average%20across%20four%20datasets%2C%20decreases)) ([Faithfulness and Accuracy: How Fine-Tuning Shapes LLM Reasoning](https://d3.harvard.edu/faithfulness-and-accuracy-how-fine-tuning-shapes-llm-reasoning/#:~:text=Faithfulness%20and%20Accuracy%3A%20How%20Fine,This%20effect%20was%20particularly)). To counteract this, researchers include math word problems in instruction tuning datasets or perform a second-stage fine-tune on math. The bottom line is: if you want strong math performance, include math in the fine-tuning mix explicitly.

In summary, the latest research suggests a combination of strategies is most effective for improving LLMs on math: **pre-train on math content, fine-tune on high-quality solutions (possibly distilled from stronger models), and apply reinforcement learning or self-training for an extra boost**. With these methods, even relatively small models like a 3B parameter Llama variant can learn to solve complex math problems that previously only the largest models could handle. Fine-tuning is key – as one benchmark showed, Llama-2 7B’s MATH score jumped from near 0 to over 40% with appropriate fine-tuning ([WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/html/2308.09583v2#:~:text=MetaMath%C2%A0%28Yu%20et%C2%A0al.%2C%202023b%20%29%20Llama,v0.1%207B%2074.8%2035.2)) ([WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/html/2308.09583v2#:~:text=Skywork,v0.1%207B%2074.8%2036.0)). Ongoing research (e.g. new datasets, better reward models for reasoning steps) continues to push these numbers higher.

## 3. Training Considerations for Llama3.2-3B on a Single GPU
Fine-tuning a model like *Llama3.2-3B* (approximately 3 billion parameters) for math reasoning requires careful consideration of hardware constraints and training techniques, especially when limited to a single GPU (Nvidia L40S with 48GB VRAM). Here we discuss the model architecture and practical tricks to maximize efficiency:

**Model and Hardware Constraints:** A 3B-parameter transformer model in full 16-bit precision will occupy roughly 6–7GB of memory for the model weights. During training, however, additional memory is needed for gradients and optimizer states (often 2–3× the model size). Naively fine-tuning a 3B model in fp32 could require ~24GB or more memory, which would fit in 48GB but leave little room for batch data or caching. Using half-precision (fp16 or bfloat16) cuts memory usage in half, so it’s strongly recommended to use mixed precision training on modern GPUs. With bfloat16 (which L40S likely supports), the model weights ~6GB and gradients similar size, making it feasible to hold the model and a decent batch within 48GB.

However, if one later scales to a 7B or 13B model, even 48GB might become a bottleneck for full fine-tuning. To prepare for that – and to generally use GPU memory efficiently – we can apply **parameter-efficient fine-tuning** methods and quantization techniques:

- **LoRA (Low-Rank Adaptation):** LoRA is a popular approach that freezes the original model weights and instead learns small “update matrices” of low rank for each weight matrix ([Noteworthy LLM Research Papers of 2024](https://sebastianraschka.com/blog/2025/llm-research-2024.html#:~:text=If%20you%20are%20finetuning%20open,efficient%20LLM)) ([Noteworthy LLM Research Papers of 2024](https://sebastianraschka.com/blog/2025/llm-research-2024.html#:~:text=large%20weight%20update%20matrix%20%CE%94W,reduces%20computational%20and%20memory%20overhead)). This drastically reduces the number of trainable parameters. For example, applying LoRA with rank $r=16$ on all dense layers of a 3B model might introduce on the order of tens of millions of new parameters (instead of 3B). In practice, LoRA often cuts GPU memory usage and compute by an order of magnitude, since only the small matrices are backpropagated. Empirical studies have found that LoRA fine-tuning achieves nearly the same performance as full fine-tuning in many cases, especially for domains that overlap with the model’s pre-training knowledge ([Noteworthy LLM Research Papers of 2024](https://sebastianraschka.com/blog/2025/llm-research-2024.html#:~:text=Full%20finetuning%20vs%20LoRA,09673)) ([Noteworthy LLM Research Papers of 2024](https://sebastianraschka.com/blog/2025/llm-research-2024.html#:~:text=5)). In fact, for mathematical tasks, the difference between LoRA and full fine-tuning is minimal – math is a domain the model has *some* familiarity with (basic concepts) so LoRA can adjust it without needing to overhaul many weights ([Noteworthy LLM Research Papers of 2024](https://sebastianraschka.com/blog/2025/llm-research-2024.html#:~:text=Less%20and%20Forgets%20Less%2C%20https%3A%2F%2Farxiv)) ([Noteworthy LLM Research Papers of 2024](https://sebastianraschka.com/blog/2025/llm-research-2024.html#:~:text=When%20examining%20how%20much%20previously,the%20difference%20is%20less%20pronounced)). An added benefit is that LoRA tends to **preserve the original model’s knowledge** better (less catastrophic forgetting) ([Noteworthy LLM Research Papers of 2024](https://sebastianraschka.com/blog/2025/llm-research-2024.html#:~:text=5)). This means a LoRA-tuned model might retain general abilities (like language fluency) while gaining math skills, whereas an aggressive full fine-tune could inadvertently forget some pre-trained knowledge. Given a single 48GB GPU, using LoRA is a wise choice – it enables larger batch sizes or larger models to be fine-tuned in memory. For instance, one can apply LoRA to a 7B or 13B model on a 48GB GPU, which might be impossible to fully fine-tune otherwise.

- **Quantization (QLoRA):** Quantizing model weights to lower precision can greatly reduce memory footprint. **QLoRA** (Quantized LoRA) is a technique introduced in 2023 that keeps the pretrained model weights in 4-bit precision during fine-tuning ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=,new%20data%20type%20that%20is)) ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=,innovations%20to%20save%20memory%20without)). The model’s forward and backward passes quantize-dequantize on the fly, and gradients are used to train LoRA adapters (typically kept in higher precision). This method allowed researchers to fine-tune a 65B model on a single 48GB GPU ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=,new%20data%20type%20that%20is)). In our case, a 3B model quantized to 4-bit would be extremely light – on the order of 1.5GB for weights. Even including LoRA matrices and optimizer overhead, that’s maybe a few GB total, which is trivial on a 48GB card. The QLoRA paper demonstrated that 4-bit fine-tuning *does not significantly hurt performance*; models retained “full 16-bit finetuning task performance” even at 4-bit ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=,new%20data%20type%20that%20is)) ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=full%2016,memory%20footprint%20by%20quantizing%20the)). So we can use QLoRA to maximize memory efficiency: load Llama3.2-3B in 4-bit precision, add LoRA adapters, and fine-tune. This leaves plenty of headroom to increase batch size (important for stability in RL training like GRPO, which benefits from comparing multiple samples). It also allows one to load a larger reward model or other components concurrently if needed.

- **Optimizer and Memory Tricks:** With limited GPU memory, it’s useful to use memory-optimized optimizers. The Hugging Face TRL example uses `adamw_8bit`, an 8-bit Adam optimizer that reduces memory usage for optimizer states ([Practical Exercise: Fine-tune a model with GRPO - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter12/5?fw=pt#:~:text=training_args%20%3D%20GRPOConfig%28%20output_dir%3D%22GRPO%22%2C%20learning_rate%3D2e,adamw_8bit)). This can cut the memory for momentum/vectors by 2× without loss in performance. Gradient checkpointing is another technique: it trades compute for memory by not storing intermediate activations, recomputing them during backpropagation. If memory is tight during training long sequences, checkpointing can enable a larger model or batch to fit. The trade-off is slower training. In our scenario, since we have a relatively comfortable 48GB for a 3B model, we may not need checkpointing, but it’s good to keep in mind if pushing limits (e.g., fine-tuning a 13B model with GRPO on this GPU – then one might checkpoint the model’s layers to fit all G=8 generated sequences’ activations).

- **Batch Size and Gradient Accumulation:** On a single GPU, large batch sizes may be constrained by memory. Mathematical reasoning data often involves long sequences (prompts + detailed solutions), which further limits how many samples fit at once. With 48GB, one might fit perhaps 8–16 sequences of a few hundred tokens each in a forward pass at fp16. To effectively increase batch size, one can use gradient accumulation – for example, accumulate gradients over 2–4 forward passes before updating weights (as shown in the HF example config with `gradient_accumulation_steps=2` ([Practical Exercise: Fine-tune a model with GRPO - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter12/5?fw=pt#:~:text=training_args%20%3D%20GRPOConfig%28%20output_dir%3D%22GRPO%22%2C%20learning_rate%3D2e,adamw_8bit))). This achieves a larger effective batch without requiring multi-GPU. It’s important for stable training of LLMs, especially in RL, to have a decent batch size to estimate advantages and KL. So, combining moderate per-device batch with accumulation is recommended.

- **Model Architecture Constraints:** The Llama3.2-3B presumably has the same architecture as Llama 2 (transformer decoder) but with fewer layers/hidden size. One constraint of smaller architectures is limited context length – if the model can only handle, say, 1024 tokens context, it might struggle with very long problem solutions. Fine-tuning can sometimes increase the average solution length (models start writing longer reasoning). We should be mindful of the max context length and possibly use techniques like prompt truncation or iterative prompting for extremely long problems. On the GPU side, longer context means more compute and memory (self-attention cost scales quadratically with sequence length), so extremely long chains-of-thought might not be efficient. In practice, most competition math solutions are within a few hundred tokens, so a 2048-token context is sufficient. If Llama3.2 came with 2048 or 4096 context, we’re fine. If not, one might consider position interpolation or fine-tuning the positional embeddings to handle longer contexts (there are methods to extend context length post-hoc).

- **Training Time:** A single L40S (roughly akin to an A100 48GB or RTX 6000 Ada) is a powerful GPU, but RL training like GRPO can be slow due to generating multiple outputs. One should expect that fine-tuning to convergence might take on the order of a day or more. The DeepSeekMath GRPO training, for instance, was done on 8 GPUs for about a day ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=Copied)), so on one GPU it could be ~8 days for similar volume (though our model is 3B vs their 7B). Utilizing accelerate or PyTorch’s FSDP is not necessary since we’re single-device, but ensuring full GPU utilization (e.g. with mixed precision and avoiding Python bottlenecks in the generation loop) will help.

In summary, **techniques like LoRA and 4-bit quantization are highly effective to fine-tune LLMs on a single GPU**. They enable larger models or larger batches without running out of memory. Empirical evidence shows LoRA achieves comparable accuracy to full-model fine-tuning on math tasks while using far less memory ([Noteworthy LLM Research Papers of 2024](https://sebastianraschka.com/blog/2025/llm-research-2024.html#:~:text=Less%20and%20Forgets%20Less%2C%20https%3A%2F%2Farxiv)) ([Noteworthy LLM Research Papers of 2024](https://sebastianraschka.com/blog/2025/llm-research-2024.html#:~:text=When%20examining%20how%20much%20previously,the%20difference%20is%20less%20pronounced)), and QLoRA preserves performance even at 4-bit precision ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=,new%20data%20type%20that%20is)) ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=enough%20to%20finetune%20a%2065B,new%20data%20type%20that%20is)). For Llama3.2-3B, this means we can easily fit the model and even a reward model on the 48GB GPU if quantized. We should freeze most of the model’s weights and only train lightweight adapters, to both save memory and prevent overfitting/forgetting. Using these methods, fine-tuning *Llama3.2-3B* for math on a single L40S GPU is not only feasible but likely efficient, allowing rapid experimentation with hyperparameters and reward functions.

## 4. Data Augmentation Strategies for Mathematical Reasoning
When training on mathematical problems, the quality and variety of the dataset are crucial. Because the first training iteration will focus on novel datasets (i.e. not augmenting existing ones yet), we review augmentation strategies to consider in subsequent rounds or to inspire dataset creation:

- **Synthetic Problem Generation (Evol-Instruct):** One powerful approach is to generate new math problems and solutions using AI itself. The WizardMath project introduced a *Math Evol-Instruct* method, where an open model generates problems through evolutionary operations ([WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/html/2308.09583v2#:~:text=match%20at%20L129%20instructions%20data,2023%29%2C%20which%20mainly%20focus)). They perform “downward evolution” to create simpler grade-school problems and “upward evolution” to create harder high-school problems from base questions ([WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/html/2308.09583v2#:~:text=match%20at%20L129%20instructions%20data,2023%29%2C%20which%20mainly%20focus)). The idea is to automatically produce a curriculum of math questions of varying difficulty. These synthetic problems come with their solutions (since the model generating them can be prompted to provide an answer or reasoning). By augmenting the training data with such generated items, you can dramatically increase the volume and diversity. Of course, one must ensure the generated problems are correct and not trivial; WizardMath addressed this by using filtering and reinforcement (reward models to favor high-quality, unique questions). In practice, an iterative approach works: generate a batch of new problems, have a verifier (like GPT-4 or a math solver) check them, keep the good ones, fine-tune the model, and repeat with the now-stronger model to get even better questions. This “self-feeding” data augmentation can rapidly bootstrap a model’s abilities. The risk is the model might generate flawed problems/solutions that, if not caught, could mislead it – so some human or reliable automated checking is needed as a filter.

- **Solution Distillation and Paraphrasing:** If you have a dataset of problems with final answers only (as many math competitions do), a great augmentation is to use a large LLM to generate full step-by-step solutions for them. This effectively creates a new training dataset of (problem, solution) pairs from an older dataset that only had (problem, answer). For example, one could take the MATH dataset’s training split (which has solutions written by humans) and have a model like GPT-4 produce an *alternative solution* for each. Now you have two solutions per problem – increasing the training data and exposing the model to different ways of reasoning. Paraphrasing can be applied to both questions and solutions. You might rephrase questions (without changing their meaning) to make the model robust to variations in wording. You can also paraphrase or reorganize solutions (e.g., a proof written in a different style). This kind of augmentation teaches the model that the underlying reasoning matters, not the superficial wording. Prior works have done similar things: rewording math problems or translating them to different formats (like turning an algebra word problem into an equation-solving task). Data augmentation via paraphrasing adds variety that helps generalization.

- **Backward Reasoning Data:** A clever augmentation is to generate problems *from answers*. For instance, take a correct solution and remove the question, then ask the model to infer a question that would be solved by that solution. This “Jeopardy-style” reversal can produce new Q&A pairs. In math, one could also start with an answer and a solution outline, and create variations of the question that lead to that answer. While this is tricky to automate perfectly, it’s a way to enrich the dataset with new problems that are guaranteed solvable by known solutions. Some researchers have used this to balance datasets – e.g., ensuring a range of final answer types or difficulty levels. Similarly, you can take an existing problem and change numerical values or conditions slightly (with known adjustments to the answer) to create a new problem. For example, if a problem asks “What is 5+7?”, changing it to “What is 6+8?” is trivial augmentation. For harder problems, one might change a parameter (like a triangle side length or an exponent) and then recompute the answer (perhaps using a CAS – computer algebra system – to ensure accuracy). This yields families of problems that train the model to solve a general pattern, not just one instance.

- **Intermediate Step Annotations:** Math problems often involve certain key steps (like applying a formula, performing an integral, etc.). Augmenting data with explicit intermediate questions can help. One strategy is to break existing solutions into smaller sub-questions and train the model on those. For example, if a solution has a step “Thus, $x=2$,” you can form a sub-problem: “Given the earlier equation, solve for x.” By augmenting the dataset with these targeted sub-steps, you teach the model to perform crucial atomic skills. Later, the model can chain them together to solve the full problem. This is related to the idea of *process supervision*, where you don’t just reward final answers but also supervise the process. For training data, it means having more granular Q&A pairs. Some projects (like *Math Shepherd* or others) have looked into labeling each step of a proof as correct/incorrect. While labor-intensive to do manually, one could use an LLM or a math solver to label steps in existing solutions (true vs false inference). These labeled steps can train a model to not only produce a solution but also verify each part. In augmentation terms, you get a dataset of “step => valid/invalid” which could be used to train a critic or to teach the main model to avoid common invalid steps.

- **Multimodal Augmentation (if applicable):** Although we focus on text, sometimes incorporating other formats can help. For example, converting equations or tabular data to text and including them in training can augment the model’s ability to handle those formats. If the model struggled with parsing LaTeX, one could augment data by taking math formulas and writing them out in words (or vice versa) to improve its versatility. However, for pure LLM fine-tuning on text math problems, this is less of a priority.

In the first training iteration, you plan to use novel datasets (presumably fresh collections of math Q&A). While you might not augment them initially, it’s wise to design these datasets with augmentation in mind. For instance, ensure a range of difficulties and problem types are represented. Later, you can focus augmentation efforts where the model is weakest (e.g., if the model struggles with geometry, you can generate more geometry questions). Also, consider **cross-pollinating datasets** – if you have a dataset of algebra problems and one of calculus problems, merging or at least jointly training on them might improve general problem-solving robustness.

One more point: **evaluation augmentation (self-consistency)** is worth noting though it’s not training data. Techniques like self-consistency (having the model generate multiple solutions at test time and pick the most common answer) can improve inference accuracy ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=code%20data,the%20memory%20usage%20of%20PPO)). While not an augmentation of training data per se, it leverages multiple reasoning paths. If a model is being fine-tuned, one could simulate this by training it to generate *multiple* solution paths in one go (if that’s feasible) or at least be aware that at inference time you might use such strategies. This means training data could include some examples of problems with multiple solution approaches, teaching the model that different reasoning can lead to the same answer.

To sum up, **effective data augmentation for math** includes generating new problems and solutions using models, rephrasing and transforming existing problems, adding intermediate step training, and using teacher models to supply high-quality rationales. These methods aim to expose the model to as many facets of math problem-solving as possible, so that it becomes proficient and not overly specialized to a narrow dataset. After an initial round of training on your novel dataset, applying these augmentations can significantly enhance the second round of training – often a critical step to push performance on tough benchmarks.

## 5. Hyperparameter Tuning for GRPO Training
Tuning hyperparameters in GRPO-based training is crucial for stable and efficient learning. Here are best practices and common pitfalls when finetuning with GRPO:

- **Group Size (G) – Number of Generations per Prompt:** This controls how many candidate completions the model produces for each prompt in one training step. A larger G provides a more reliable relative ranking of outputs (better estimate of the “best” vs “worst” answer), which can improve the advantage computation. However, generating many outputs per prompt is computationally expensive. In practice, values like G = 4 or 8 are common compromises ([Practical Exercise: Fine-tune a model with GRPO - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter12/5?fw=pt#:~:text=learning_rate%3D2e,num_train_epochs%3D1)) ([Practical Exercise: Fine-tune a model with GRPO - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter12/5?fw=pt#:~:text=max_completion_length%3D96%2C%20num_generations%3D8%2C%20optim%3D,remove_unused_columns%3DFalse%2C%20logging_steps%3D1)). Hugging Face’s examples use `num_generations=8` (meaning 8 completions per prompt) ([Practical Exercise: Fine-tune a model with GRPO - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter12/5?fw=pt#:~:text=learning_rate%3D2e,num_train_epochs%3D1)). For a math task, G=4 might suffice since the reward (correct vs incorrect) is often very distinct – you just need at least one good and one bad sample to get a meaningful comparison. If G is too low (e.g. 2), you might occasionally sample two equally bad outputs and the model gets almost zero learning signal that step. Too high G (like 16) will slow training greatly. **Best practice:** start with G ~ 4 and see if the advantage estimates seem noisy; increase to 8 if needed and if compute allows. Ensure that your batch size times G isn’t so high that it overflows memory – better to increase batch or G, but not both excessively.

- **KL Coefficient (β):** This hyperparameter weights the KL-divergence penalty term in the loss ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=The%20objective%20is%20to%20maximize,i%2Ct%E2%88%92%CE%B2%20D%20KL%5B%CF%80%20%CE%B8%E2%88%A5%CF%80%20ref)) ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=vllm_guided_decoding_regex%3A%20typing.Optional,6%20ref_model_sync_steps%3A%20int%20%3D%20512)). β determines how strongly the model is pulled back towards the reference (original policy). A higher β means the model changes more conservatively (stays closer to initial behavior), a lower β lets it stray further to chase reward. In the GRPOConfig, `beta` default is 0.04 ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=vllm_guided_decoding_regex%3A%20typing.Optional,6%20ref_model_sync_steps%3A%20int%20%3D%20512)). Tuning β is important: if you find the model’s outputs are becoming nonsensical or diverging, you might increase β to regularize it more. If the model is learning too slowly or not improving reward, you might slightly decrease β to let it explore. **Common pitfall:** setting β = 0 (disabling the KL penalty) – this usually leads to instability and the model going off-distribution, as noted in the docs (β=0.0 not loading a ref model can be numerically unstable for long runs) ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=match%20at%20L728%20,%E2%80%94%20Number%20of%20iterations%20per)). Another sign of β being too low is the KL divergence metric blowing up (meaning the new policy is far from the old one). It’s good to monitor `KL_divergence` during training; many RLHF setups aim for a target KL (like ~5-10 nats) and adjust β dynamically to maintain that. In GRPO’s simpler implementation, you might do manual sweeps: e.g. try β = 0.02, 0.05, 0.1 and see which yields the best final reward without output degradation.

- **Clipping Parameter (ε):** Like PPO, GRPO can use a clipped objective to avoid overly large policy updates ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=In%20the%20original%20paper%2C%20this,i%2Ct%29%E2%88%92%CE%B2%20D%20KL%5B%CF%80%20%CE%B8%E2%88%A5%CF%80%20ref)). The epsilon (ε) is typically 0.1 or 0.2 in PPO. GRPOConfig default ε = 0.2 ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=vllm_guided_decoding_regex%3A%20typing.Optional,6%20ref_model_sync_steps%3A%20int%20%3D%20512)). If using multiple update iterations (`num_iterations > 1`), clipping is definitely needed to prevent divergence. In most cases with GRPO (especially if `num_iterations=1`, i.e. one gradient update per batch of new samples), clipping might rarely be activated if your learning rate is reasonable. But it’s a safeguard. **Tuning advice:** you generally don’t need to adjust ε unless you observe instability. If the training is oscillating or diverging even with a low learning rate and moderate β, consider reducing ε (say to 0.1) to make updates even more conservative. On the other hand, if you set `num_iterations` (the inner PPO-like epochs on the same data) to >1 for efficiency, ensure ε is standard (0.2) so that multiple updates don’t drift too far ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=GRPOConfig,i%2Ct%29%E2%88%92%CE%B2%20D%20KL%5B%CF%80%20%CE%B8%E2%88%A5%CF%80%20ref)).

- **Learning Rate and Optimizer:** A too-high learning rate is a common cause of RL training instability. Since GRPO is an on-policy method, you typically use a smaller LR than in supervised finetuning. The example uses 2e-5 ([Practical Exercise: Fine-tune a model with GRPO - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter12/5?fw=pt#:~:text=training_args%20%3D%20GRPOConfig%28%20output_dir%3D%22GRPO%22%2C%20learning_rate%3D2e,adamw_8bit)), which is in the ballpark. Depending on model size, one might go as high as 1e-4 or as low as 1e-6. For a 3B model, 1e-5 to 5e-5 is a reasonable range to try. If you see the reward spiking then collapsing, or loss exploding, that signals LR too high (try lowering by 10x). Optimizer choice: AdamW is standard, and using an 8-bit optimizer is fine (it doesn’t usually affect tuning beyond memory). One specific hyperparam to watch is **max gradient norm** (grad clipping). Often set to 1.0 by default in transformers, this prevents huge gradient updates ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=1e,transformers.trainer_utils.SchedulerType%2C%20str%5D%20%3D%20%27linear)). It’s good to leave this on; if you find gradients are often getting clipped, that might mean LR is a bit high or your reward signal is very sparse (leading to occasional huge advantage values).

- **Reward Scaling:** Although not an explicit hyperparameter in code, how you scale/shape your reward function matters. If your reward outputs are very large or unbounded, it can affect training dynamics. For math, a binary correct/incorrect can be cast to say +1 and 0 rewards. GRPO will normalize within the group (A = (r_i – mean)/std), so even binary rewards become something like [+1, -0.5, -0.5, ...] if one is correct and others not, for example. That normalization helps, but you should ensure the reward model or function yields rewards on a consistent scale. If using a learned reward model, check that its scores are not too skewed or drifting during training (if you also update the reward model, that’s another complexity – usually reward models are fixed to a separate pretraining). Best practice: keep reward signal simple and bounded. Some implementations even clip or tanh the reward to avoid outliers. Monitor the `reward_mean` and `reward_std` (logged by GRPOTrainer) ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=Logged%20metrics)) to ensure they are reasonable (reward_std shouldn’t be zero or huge; mean should gradually increase if learning).

- **Reference Model Update:** By default, the reference policy in GRPO (used for KL) is the initial model kept static. There is a hyperparam `sync_ref_model` (and related `ref_model_sync_steps`) in the config ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=vllm_guided_decoding_regex%3A%20typing.Optional,6%20ref_model_sync_steps%3A%20int%20%3D%20512)). If `sync_ref_model=False` (default), the ref model is fixed. If set to True, the library will periodically update the reference model to the current model (or a mix). This effectively “moves the goalposts,” preventing KL from endlessly growing. Some have found that in long training runs, periodically syncing the reference to the latest policy (with some mixing factor α) can help continue improving without hitting a KL wall. The default behavior is usually fine for short runs, but if you plan to train through many iterations such that the model changes a lot, consider enabling `sync_ref_model` so that KL is always measured against a recent policy rather than the very initial one. This way, β=0.04 keeps the model close to the recent past, rather than anchored to the starting model forever. The risk of syncing is that you can get some drift in style – you’re essentially doing an *unconstrained* optimization if you sync too often (since the reference will follow the model). A middle ground is “mixing reference” where reference is a moving average of past policies (controlled by `ref_model_mixup_alpha`). If these options are exposed, they can be tuned (α=0.6 default ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=vllm_guided_decoding_regex%3A%20typing.Optional,6%20ref_model_sync_steps%3A%20int%20%3D%20512))). But if unsure, it’s safe to leave ref model fixed for one pass, then possibly do a second RL run starting from that new model if more optimization is needed.

- **Epochs/Iterations:** In on-policy RL like GRPO, typically you train until the reward plateaus or starts decreasing (indicating over-optimization). There isn’t a hard concept of epochs (since new data is generated each “epoch”). However, if you have a finite prompt dataset, you may loop through it multiple times. The `num_train_epochs` in GRPOConfig might refer to cycling through the prompt set. A prompt can be reused with different sampled outputs each epoch, which is fine. Ensure you shuffle prompts if so. Often, a few epochs (1-3) over a prompt set are enough, because each prompt generates many outputs over training. **Watch for overfitting:** if the model starts memorizing specific prompts/answers (especially if your prompt dataset is small), you might see reward continue to increase but generalization suffer. Avoid running too many epochs; it’s usually better to have more prompts or to introduce some variation (like augment prompts or add a bit of randomness in them) than to over-loop on a small set. Early stopping based on validation performance (if you have some held-out math problems) can be useful. For example, periodically test the model on a sample of MATH problems; if accuracy stops improving or begins dropping while the training reward still rises, you might be overfitting to the reward model. That’s a known pitfall: *reward hacking*. The model might find ways to please the reward model (or satisfy the programmed reward) that don’t actually translate to better real accuracy. Keeping an eye on a real metric (like solve rate on a mini-benchmark) guards against this.

- **Common Pitfalls and Optimization Tricks:**
  - *Mode collapse:* In extreme cases, the model might converge to a degenerate strategy that yields high reward. For instance, if the reward function only checks final numeric correctness, the model might learn to output just the final number without any reasoning (if it figures out that gets reward). This is not ideal if we want full solutions. We can counteract this by designing the reward to also value the reasoning format (e.g., require certain reasoning steps, or use a reward model that looks at the whole solution). Indeed, DeepSeekMath used a mix of rewards – some for accuracy, some for format ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=reinforcement%20learning%20on%20their%20base,like%20format%2C%20mathematics%2C%20and%20coding)). If you notice the model dropping all explanation and just guessing answers, consider augmenting the reward: e.g., give a small positive reward for showing work (and zero if not), in addition to the correctness reward ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=reinforcement%20learning%20on%20their%20base,like%20format%2C%20mathematics%2C%20and%20coding)) ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=,between%20%E2%80%98%E2%80%99%20and%20%E2%80%98%E2%80%99%20tags)). This encourages the behavior you want.
  - *High variance in rewards:* If only a few prompts in your batch have a non-zero reward (e.g. only occasionally the model finds a correct solution), training can be slow. You might address this by reward shaping – give partial credit. For math, maybe reward the model for each correct step or intermediate result, not just the final answer. This makes the advantage less sparse. The Self-Explore technique is an example of more granular reward signals (rewarding avoiding wrong steps) ([Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards](https://arxiv.org/html/2404.10346v4#:~:text=To%20this%20end%2C%20we%20propose,Our%20code%20is%20available%20here)) ([Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards](https://arxiv.org/html/2404.10346v4#:~:text=Inspired%20by%20prior%20works%20that,the%20rationales%20into%20positive%20and)). Tuning the granularity of reward can be seen as a hyperparam: too sparse and learning signal is weak; too dense and the model might optimize sub-tasks without truly solving end-to-end.
  - *Monitoring and adjusting:* Keep an eye on training curves. Ideally log: average reward per prompt, std of reward (to see if groups are usually getting one good vs one bad, etc.), KL divergence, and maybe the ratio of tokens changed (some measure of how much the policy is updating). If reward plateaus early and KL remains very low, you might loosen constraints (lower β or raise LR) to push it more. If reward plateaus and KL is rising steadily, you might have hit the limit of the current reward model or dataset – maybe then stop training or switch to another method (like do a supervised fine-tune on the newly generated high-reward outputs, as a form of distillation).
  - *Length and diversity:* One hyperparam in generation is the **max completion length** (how long outputs can be). If too short, the model may be cut off before finishing solutions, confusing the reward. If too long, the model might ramble and possibly get penalized by KL for extra length. Setting a reasonable max tokens for the completions (maybe a bit longer than the longest solution in your data) is wise ([Practical Exercise: Fine-tune a model with GRPO - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter12/5?fw=pt#:~:text=per_device_train_batch_size%3D8%2C%20gradient_accumulation_steps%3D2%2C%20max_prompt_length%3D512%2C%20max_completion_length%3D96%2C%20num_generations%3D8%2C,num_train_epochs%3D1%2C%20bf16%3DTrue)). Also use an appropriate decoding strategy: typically sampling with a moderate temperature to get diverse outputs for GRPO. The trainer likely does this internally. Temperature or top-p can be considered hyperparams too – if outputs are too deterministic, you won’t explore different solutions; if too random, many outputs will be garbage. A temp around 1.0 is usually used during RL training for diversity.

- **Hyperparameter tuning process:** It’s often useful to run short experiments on a subset of data to tune these. For example, try a 1-epoch run with different β values to see which yields a stable increase in reward. Or test different learning rates on a small sample of prompts. Because full GRPO runs can be lengthy, doing A/B comparisons on smaller scales can guide the settings for the full run.

In summary, **start with sane defaults (β ~0.02–0.05, ε=0.2, moderate LR, G=4–8)** and adjust based on observations. The main things to tune are the balance between reward optimization and staying general (handled by β and perhaps reference syncing), and the aggressiveness of updates (LR, clipping). Avoid the pitfall of “over-optimizing” the reward at the cost of actual task performance – always validate on real math problems if possible. Finally, consider a schedule for some hyperparams: e.g., some implementations gradually increase β over time to tighten the policy to the reference after an initial exploration phase. Such schedules or adaptive tuning (as done in some PPO implementations to keep KL in a target range) can be advanced strategies if manual tuning is challenging.

## 6. Model Evaluation on the MATH Benchmark
The **MATH benchmark** (introduced by Hendrycks et al., 2021) is a standard for evaluating an LLM’s mathematical problem-solving ability ([[2103.03874] Measuring Mathematical Problem Solving With the MATH Dataset](https://arxiv.org/abs/2103.03874#:~:text=,show%20that%20accuracy%20remains%20relatively)). It consists of 12,500 competition-level math problems drawn from contests like AMCs, AIME, and other high school Olympiad-style examinations ([[2103.03874] Measuring Mathematical Problem Solving With the MATH Dataset](https://arxiv.org/abs/2103.03874#:~:text=,show%20that%20accuracy%20remains%20relatively)). Each problem is a word problem covering topics such as algebra, geometry, number theory, calculus, etc., and they are categorized by difficulty level (Level 1 easiest up to Level 5 hardest). Importantly, each problem in MATH comes with a **complete step-by-step solution** in the dataset, which can be used for training or just for reference ([[2103.03874] Measuring Mathematical Problem Solving With the MATH Dataset](https://arxiv.org/abs/2103.03874#:~:text=,show%20that%20accuracy%20remains%20relatively)). However, for evaluation, models are typically only given the problem statement (without solution) and asked to produce the answer.

**Evaluation Protocol:** MATH is usually treated as a text-generation task where the model must eventually provide a final answer (often a numeric or simplified expression). The official metric is the percentage of problems answered correctly (exact match to the provided answer) by the model’s solution. Because the model might generate a full solution, one needs to extract the final answer from it to check correctness. The benchmark is quite unforgiving: the answer has to be exactly correct. If a model’s reasoning is sound but it makes an arithmetic slip at the end, that problem is marked wrong. Some evaluations allow a few attempts (for instance, *pass@k* metrics, where the model can give k answers or solutions and it counts as correct if any contain the right answer). DeepSeekMath reported both pass@1 and self-consistency (64 samples majority vote) results ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=code%20data,the%20memory%20usage%20of%20PPO)), showing that generating multiple solutions and voting improved accuracy from 51.7% to 60.9% on MATH ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=code%20data,the%20memory%20usage%20of%20PPO)). But usually, for a fair comparison, we use pass@1 (one try, model’s final answer).

**Why MATH is Challenging:** These problems often require multiple reasoning steps and cover advanced high school topics. They are much harder than simple arithmetic or common word problems. Early language models struggled on MATH – for example, GPT-3 (175B) reportedly solved only around 6% of MATH problems correctly ([[2103.03874] Measuring Mathematical Problem Solving With the MATH Dataset](https://arxiv.org/abs/2103.03874#:~:text=accuracy%20on%20MATH%2C%20we%20also,more%20traction%20on%20mathematical%20problem)). Even as models scaled, performance remained low without special training: Hendrycks et al. noted that “scaling is not currently solving MATH” – a 13B model and a 175B model were both under 10% if not specifically fine-tuned ([[2103.03874] Measuring Mathematical Problem Solving With the MATH Dataset](https://arxiv.org/abs/2103.03874#:~:text=accuracy%20on%20MATH%2C%20we%20also,more%20traction%20on%20mathematical%20problem)). This made MATH a great benchmark to test advanced reasoning and the effect of fine-tuning strategies. Achieving high scores on MATH usually requires the model to do algebraic manipulation, logical deductions, and sometimes clever insights (the kind a human contest solver would need).

**Use in Evaluation:** When you fine-tune Llama3.2-3B for math, you’ll likely evaluate it on MATH’s test set to gauge progress. Typically, one uses the MATH test split (which has problems but usually not solutions, to prevent training on them) and prompts the model with each problem (possibly with a standard instruction like “Solve this step by step and give the answer:” to encourage chain-of-thought). Then the model’s final answer is compared to the ground truth answer. Because MATH has many problems, accuracy on this test is a robust measure – it’s hard to overfit due to sheer volume and variety. Researchers also sometimes report performance by difficulty level or by topic to understand strengths and weaknesses.

**Comparison with Other Benchmarks:** It’s worth noting that there are other math benchmarks too – *GSM8K* (grade school math, 8.5K problems) is easier and many models have >90% on it now. *MATH* is significantly harder; a score above 50% on MATH is considered excellent for an open model. State-of-the-art closed models like GPT-4 score around 80%+ on MATH (WizardMath mentions GPT-4 at 76.6% on MATH as of mid-2023 ([WizardMath: Empowering Mathematical Reasoning for Large ...](https://openreview.net/forum?id=mMPMHWOdOy#:~:text=WizardMath%3A%20Empowering%20Mathematical%20Reasoning%20for,)), and likely GPT-4 has improved since). The best open 7B-ish models (as of late 2024) are now in the 70% range on MATH ([WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/html/2308.09583v2#:~:text=JiuZhang3.0%C2%A0%28Zhou%20et%C2%A0al.%2C%202024%20%29%20Mistral,hdashline)), which shows rapid progress.

**Assessment of Fine-Tuning Strategies:** When evaluating different fine-tuning strategies using MATH, you often see clear differences. For example, a model fine-tuned only on chain-of-thought will do better on MATH than one fine-tuned only to give a direct answer, because the former is more capable of multi-step reasoning. A model that underwent RL (like GRPO) might show strengths in tricky problems where it learned to double-check its work. MATH is difficult enough that any flawed strategy will show: models might get easy questions right but fail all hard ones, or make systematic mistakes (like always giving a numeric guess without reasoning – which often is wrong). So, it’s common to analyze *where* in MATH a model is doing well or poorly. For instance, DeepSeekMath’s high score suggested it was solving even many level-5 problems, not just the easy ones ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=code%20data,enhances%20mathematical%20reasoning%20abilities%20while)). If your Llama3.2-3B fine-tune gets, say, 30% on MATH, you might inspect its performance by topic: maybe it does well in algebra but poorly in geometry, indicating a need for more geometry training data.

**Using MATH for validation:** Because MATH is large, you might carve out a subset as a validation set during development. Ensure you don’t train on the official test problems if you plan to report that score. Some approaches also use MATH’s official training set (which has ~10K problems with solutions) as part of fine-tuning data – that’s fine, but then they evaluate on the test set of MATH. Given that your focus is on new datasets and GRPO, you might not directly train on MATH’s own data initially, making the test a truly blind evaluation of how well the model generalized its math skills.

In sum, **the MATH benchmark is the go-to evaluation for advanced math reasoning in LLMs**, providing a rigorous test across many domains of math. A strong result on MATH indicates the model can handle complex, competition-style problems. For your project, a good target would be pushing Llama3.2-3B’s MATH accuracy as high as possible (perhaps comparing baseline DPO tuning vs GRPO tuning). Any improvement on the MATH score directly reflects better mathematical reasoning. Moreover, methods like self-consistency (having the model generate multiple answers and taking a majority vote) can be applied at eval time to possibly boost the score (DeepSeekMath did this to jump from 51.7% to 60.9% ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=code%20data,enhances%20mathematical%20reasoning%20abilities%20while))). But when comparing different training methods, it’s fair to compare their pass@1 accuracy. Ultimately, success will be measured by how close the model gets to a high accuracy on MATH without external tools – a challenge that GRPO and fine-tuning innovations are helping to tackle.

**Sources:**

1. Shao et al., *“DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,”* arXiv 2024 ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=code%20data,the%20memory%20usage%20of%20PPO)).
2. Philschmid Blog, *“Bite: How DeepSeek R1 was trained,”* 2025 ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=Group%20Relative%20Policy%20Optimization%20,to%20improve%20models%20on%20helpfulness)) ([Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1#:~:text=The%20Key%20Differences%20from%20Proximal,PPO%29%20are)).
3. Hugging Face TRL Docs – *GRPO Trainer* ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=The%20objective%20is%20to%20maximize,i%2Ct%E2%88%92%CE%B2%20D%20KL%5B%CF%80%20%CE%B8%E2%88%A5%CF%80%20ref)) ([GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#:~:text=vllm_guided_decoding_regex%3A%20typing.Optional,6%20ref_model_sync_steps%3A%20int%20%3D%20512)) and GRPO example ([Practical Exercise: Fine-tune a model with GRPO - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter12/5?fw=pt#:~:text=training_args%20%3D%20GRPOConfig%28%20output_dir%3D%22GRPO%22%2C%20learning_rate%3D2e,num_train_epochs%3D1)).
4. Xu et al., *“Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study,”* arXiv 2024 (via Sebastian Raschka’s summary) ([Noteworthy LLM Research Papers of 2024](https://sebastianraschka.com/blog/2025/llm-research-2024.html#:~:text=Is%20DPO%20Superior%20to%20PPO,distribution%20data)) ([Noteworthy LLM Research Papers of 2024](https://sebastianraschka.com/blog/2025/llm-research-2024.html#:~:text=widely%20used%20in%20practice%20than,PPO)).
5. Biderman et al., *“LoRA Learns Less and Forgets Less,”* arXiv 2024 (via Raschka’s blog) ([Noteworthy LLM Research Papers of 2024](https://sebastianraschka.com/blog/2025/llm-research-2024.html#:~:text=There%20are%20some%20more%20nuances%2C,in%20terms%20of%20learning%20capacity)) ([Noteworthy LLM Research Papers of 2024](https://sebastianraschka.com/blog/2025/llm-research-2024.html#:~:text=When%20examining%20how%20much%20previously,the%20difference%20is%20less%20pronounced)).
6. Dettmers et al., *“QLoRA: Efficient Finetuning of Quantized LLMs,”* arXiv 2023 ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=,new%20data%20type%20that%20is)) ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=enough%20to%20finetune%20a%2065B,memory%20footprint%20by%20quantizing%20the)).
7. Hwang et al., *“Self-Explore: Fine-Grained Rewards for Mathematical Reasoning,”* Findings of ACL 2024 ([Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards](https://arxiv.org/html/2404.10346v4#:~:text=problem%20of%20whether%20LLMs%20could,Our%20code%20is%20available%20here)) ([Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards](https://arxiv.org/html/2404.10346v4#:~:text=To%20this%20end%2C%20we%20propose,Our%20code%20is%20available%20here)).
8. Luo et al., *“WizardMath: Empowering Mathematical Reasoning via Reinforced Evol-Instruct,”* arXiv 2023 ([WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/html/2308.09583v2#:~:text=This%20paper%20introduces%20WizardMath%2C%20a,supervision%20in%20achieving%20exceptional%20performance)) ([WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/html/2308.09583v2#:~:text=Skywork,v0.1%207B%2074.8%2036.0)).
9. Hendrycks et al., *“Measuring Mathematical Problem Solving with the MATH Dataset,”* NeurIPS 2021 ([[2103.03874] Measuring Mathematical Problem Solving With the MATH Dataset](https://arxiv.org/abs/2103.03874#:~:text=,show%20that%20accuracy%20remains%20relatively)).
